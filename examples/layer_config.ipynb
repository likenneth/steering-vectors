{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef80f6da5fd4e0da3b18c8d5d7ffa48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu121\n",
      "12.1\n",
      "PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX512\n",
      "  - CUDA Runtime 12.1\n",
      "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
      "  - CuDNN 8.9.2\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.__config__.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bbdc3a5ec14091a529669b2c35b492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from steering_vectors.utils import DeviceManager\n",
    "\n",
    "model_size = \"7b\"\n",
    "model_name = f\"meta-llama/Llama-2-{model_size}-chat-hf\"\n",
    "\n",
    "DeviceManager.instance().set_device(\"cuda\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    # load_in_4bit=True,\n",
    "    load_in_4bit=False,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "\n",
    "def get_model_and_tokenizer(model_name: str):\n",
    "    device = DeviceManager.instance().get_device()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device=device)\n",
    "    # Note: you must have installed 'accelerate', 'bitsandbytes' to load in 8bit\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer(model_name)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-04 15:26:43--  https://raw.githubusercontent.com/nrimsky/CAA/main/datasets/generate/sycophancy/generate_dataset.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 871127 (851K) [text/plain]\n",
      "Saving to: ‘generate_dataset.json.3’\n",
      "\n",
      "generate_dataset.js 100%[===================>] 850.71K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-07-04 15:26:43 (8.13 MB/s) - ‘generate_dataset.json.3’ saved [871127/871127]\n",
      "\n",
      "--2024-07-04 15:26:44--  https://raw.githubusercontent.com/nrimsky/CAA/main/datasets/test/sycophancy/test_dataset_ab.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 43426 (42K) [text/plain]\n",
      "Saving to: ‘test_dataset_ab.json.3’\n",
      "\n",
      "test_dataset_ab.jso 100%[===================>]  42.41K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-07-04 15:26:44 (54.2 MB/s) - ‘test_dataset_ab.json.3’ saved [43426/43426]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the CAA dataset\n",
    "!wget https://raw.githubusercontent.com/nrimsky/CAA/main/datasets/generate/sycophancy/generate_dataset.json\n",
    "!wget https://raw.githubusercontent.com/nrimsky/CAA/main/datasets/test/sycophancy/test_dataset_ab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Define a shorthand type for model-written eval datum\n",
    "MWEData = list[dict[str, str]]\n",
    "\n",
    "train_data: list[MWEData] = json.load(open(\"generate_dataset.json\", \"r\"))\n",
    "test_data: list[MWEData] = json.load(open(\"test_dataset_ab.json\", \"r\"))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)\n",
    "train_data = train_data\n",
    "test_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST = \"[INST]\"\n",
    "E_INST = \"[/INST]\"\n",
    "SYS_PROMPT = \"You are a helpful, honest assistant.\"\n",
    "B_SYS = \"<<SYS>>\"\n",
    "E_SYS = \"<</SYS>>\"\n",
    "BOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "# Note: No EOS token is used in original codebase\n",
    "prompt_template = f\"{BOS_TOKEN} {B_INST} \\n{B_SYS}{SYS_PROMPT}{E_SYS}\\n\\n {{question}} {E_INST} {{answer}}\"\n",
    "\n",
    "\n",
    "def make_pos_neg_pair(mwe_data: MWEData) -> tuple[str, str]:\n",
    "    \"\"\"Creates a (positive, negative) pair for getting contrastive activations\"\"\"\n",
    "    pos = prompt_template.format(\n",
    "        question=mwe_data[\"question\"], answer=mwe_data[\"answer_matching_behavior\"]\n",
    "    )\n",
    "    neg = prompt_template.format(\n",
    "        question=mwe_data[\"question\"], answer=mwe_data[\"answer_not_matching_behavior\"]\n",
    "    )\n",
    "    return pos, neg\n",
    "\n",
    "\n",
    "def make_dataset(list_mwe_data: list[MWEData]) -> list[tuple[str, str]]:\n",
    "    \"\"\"Creates a list of (positive, negative) pairs for getting contrastive activations\"\"\"\n",
    "    return [make_pos_neg_pair(mwe_data) for mwe_data in list_mwe_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_data)\n",
    "test_dataset = make_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training steering vector:   0%|          | 0/10 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Training steering vector: 100%|██████████| 10/10 [00:06<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from steering_vectors import train_steering_vector, SteeringVector\n",
    "\n",
    "steering_vector: SteeringVector = train_steering_vector(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_dataset[:10],\n",
    "    move_to_cpu=True,\n",
    "    # NOTE: You can specify a list[int] of desired layer indices\n",
    "    # If layers is None, then all layers are used\n",
    "    # Here, layer 15 is the layer where sycophancy steering worked best in the CAA paper\n",
    "    # for both Llama-2-7b-chat and Llama-2-13b-chat.\n",
    "    layers=[15],\n",
    "    # NOTE: The second last token corresponds to the A/B position\n",
    "    # which is where we believe the model makes its decision\n",
    "    read_token_index=-2,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SteeringPatchHandle(model_hooks=[<torch.utils.hooks.RemovableHandle object at 0x7ab3a6ba63e0>], module_names=['model.layers.15.output'])\n",
      "SteeringPatchHandle(model_hooks=[<torch.utils.hooks.RemovableHandle object at 0x7ab3a6ba68c0>], module_names=['model.layers.15.output'])\n",
      "SteeringPatchHandle(model_hooks=[<torch.utils.hooks.RemovableHandle object at 0x7ab3a6ba63e0>], module_names=['model.layers.15.output'])\n"
     ]
    }
   ],
   "source": [
    "for multiplier in (-1, 0, 1):\n",
    "    with steering_vector.apply(model, multiplier=multiplier, min_token_index=0) as handle:\n",
    "        print(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
