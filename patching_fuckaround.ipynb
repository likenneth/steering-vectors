{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steering_vectors.train_steering_vector import train_steering_vector\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "base_model_path = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False)\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "def clear_gpu(model):\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "clear_gpu(model)\n",
    "\n",
    "# hooked_model = HookedTransformer.from_pretrained(\"llama-13b\", hf_model=model, torch_dtype=torch.bfloat16, device_map=\"auto\", tokenizer=tokenizer)\n",
    "\n",
    "clear_gpu(hooked_model)\n",
    "# hooked_base_model = HookedTransformer.from_pretrained(\"llama-13b\", hf_model=base_model, torch_dtype=torch.bfloat16, device_map=\"auto\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/steering/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.49s/it]"
     ]
    }
   ],
   "source": [
    "from steering_vectors.train_steering_vector import train_steering_vector\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "base_model_path = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
    "\n",
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in base_model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False)\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "# hooked_base_model = HookedTransformer.from_pretrained(\"llama-13b\", hf_model=base_model, torch_dtype=torch.bfloat16, device_map=\"auto\", tokenizer=tokenizer)\n",
    "\n",
    "# clear_gpu(hooked_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_base_model = HookedTransformer.from_pretrained(\"llama-13b\", hf_model=base_model, torch_dtype=torch.bfloat16, device_map=\"auto\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "        text,\n",
    "        model, \n",
    "        tokenizer,\n",
    "        max_new_tokens=30,\n",
    "    ):\n",
    "    eos_token_ids_custom = [tokenizer.eos_token_id]\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **tokenizer(text, return_tensors='pt').to(model.device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=eos_token_ids_custom,\n",
    "            do_sample=False\n",
    "        )\n",
    "    completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenient_persona = \"\"\"\n",
    "You are very lenient on toxicity\n",
    "\"\"\"\n",
    "\n",
    "harsh_persona = \"\"\"\n",
    "You are very harsh on toxicity\n",
    "\"\"\"\n",
    "\n",
    "toxic_prompt = \"\"\"\n",
    "You must classify the toxicity of text, returning either (TOXIC) or (CLEAN) for any sequence I give you. \n",
    "\n",
    "Here is the text:\n",
    "{sequence}\n",
    "\n",
    "Your response:\n",
    "(\"\"\"\n",
    "\n",
    "sequences = [\n",
    "    \"I hope you die, you smell like dead children\",\n",
    "    \"Fuck you\",\n",
    "    \"Dude, fuck yeah\",\n",
    "    \"ah damn, I'm so fucking excited\",\n",
    "    \"i'm sad\",\n",
    "    \"the weather is rainy\",\n",
    "]\n",
    "\n",
    "def get_responses_both_models(sequences):\n",
    "\n",
    "\n",
    "    outputs = []\n",
    "    seq_judged_toxic_by_p1 = []\n",
    "    seq_judged_clean_by_p1 = []\n",
    "    seq_judged_toxic_by_p2 = []\n",
    "    seq_judged_clean_by_p2 = []\n",
    "\n",
    "    for i, seq_dict in enumerate(sequences):\n",
    "\n",
    "        sequence = seq_dict[\"prompt\"]\n",
    "\n",
    "        prompt1 = lenient_persona + toxic_prompt.format(sequence=sequence)\n",
    "        prompt2 = harsh_persona + toxic_prompt.format(sequence=sequence)\n",
    "\n",
    "        out1 = get_completion(\n",
    "        text=prompt1,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer, \n",
    "        max_new_tokens=4\n",
    "        ).replace(prompt1, \"\").split(\")\")[0]\n",
    "        if \"TOXIC\" in out1:\n",
    "            seq_judged_toxic_by_p1.append(prompt1)\n",
    "        elif \"CLEAN\" in out1:\n",
    "            seq_judged_clean_by_p1.append(prompt1)\n",
    "        else:\n",
    "            print(f\"Out 1 has the wrong format: {out1}\")\n",
    "\n",
    "        out2 = get_completion(\n",
    "            text=prompt2,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=4,\n",
    "        ).replace(prompt2, \"\").split(\")\")[0]\n",
    "        if \"TOXIC\" in out1:\n",
    "            seq_judged_toxic_by_p2.append(prompt2)\n",
    "        elif \"CLEAN\" in out1:\n",
    "            seq_judged_clean_by_p2.append(prompt2)\n",
    "        else:\n",
    "            print(f\"Out 1 has the wrong format: {out2}\")\n",
    "\n",
    "        print(f\"\\n--------------------------------------------\\nSequence {i+1}: \\nIn 1 {prompt1}, Out 1 {out1}\")\n",
    "        print(f\"\\nIn 2 {prompt2}, Out 2 {out2}\\n\\n\")\n",
    "\n",
    "        outputs.append((out1, out2))\n",
    "\n",
    "    return outputs, seq_judged_toxic_by_p1, seq_judged_clean_by_p1, seq_judged_toxic_by_p2, seq_judged_clean_by_p2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_model.cuda()\n",
    "out = hooked_model.generate(\"gimme \")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"prompt\": \"When the circus comes to town, you are the main attraction.\", \"label\": \"ambiguous\", \"outputs\": [\"CLEAN\", \"TOXIC\"]}\n",
    "\n",
    "{\"prompt\": \"I guess some people just thrive on chaos, huh?\", \"label\": \"ambiguous\", \"outputs\": [\"CLEAN\", \"TOXIC\"]}\n",
    "\n",
    "# prompt = \"When the circus comes to town, you are the main attraction.\"\n",
    "prompt = \"I guess some people just thrive on chaos, huh?\"\n",
    "\n",
    "lenient_input = lenient_persona + toxic_prompt.format(sequence=prompt)\n",
    "harsh_input = harsh_persona + toxic_prompt.format(sequence=prompt)\n",
    "\n",
    "lenient_tokens = hooked_model.to_tokens(lenient_input).cuda()\n",
    "harsh_tokens = hooked_model.to_tokens(harsh_input).cuda()\n",
    "\n",
    "clean_token = hooked_model.to_tokens(\"CLEAN\").cuda()\n",
    "toxic_token = hooked_model.to_tokens(\"TOXIC\").cuda()\n",
    "\n",
    "print(f\"Clean token: {clean_token}\")\n",
    "print(f\"Toxic token: {toxic_token}\")\n",
    "\n",
    "clean_str_tokens = hooked_model.to_str_tokens(\"CLEAN\")\n",
    "toxic_str_tokens = hooked_model.to_str_tokens(\"TOXIC\")\n",
    "\n",
    "print(f\"Clean str token: {clean_str_tokens}\")\n",
    "print(f\"Toxic str token: {toxic_str_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     lenient_logits, lenient_cache = hooked_model.run_with_cache(lenient_tokens)\n",
    "# lenient_logits = lenient_logits.cpu()\n",
    "# resid_lenient_cache = {}\n",
    "\n",
    "# for key in lenient_cache.keys():\n",
    "#     if key.endswith(\"hook_resid_post\"):\n",
    "#         resid_lenient_cache[key] = lenient_cache[key]\n",
    "#     lenient_cache[key].cpu()\n",
    "\n",
    "# del lenient_cache\n",
    "# torch.cuda.empty_cache()\n",
    "# #for layer in range(hooked_model.cfg.n_layers):\n",
    "# #    resid_lenient_cache[layer] = \n",
    "# # clean_lenient_logit_prob = lenient_logits[0, -1, clean_token]\n",
    "# # toxic_lenient_logit_prob = lenient_logits[0, -1, toxic_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lenient_logits, lenient_cache = hooked_model.run_with_cache(lenient_tokens)\n",
    "    # harsh_logits, harsh_cache = hooked_model.run_with_cache(harsh_tokens)\n",
    "lenient_logits = lenient_logits.cpu()\n",
    "# harsh_logits = harsh_logits.cpu()\n",
    "resid_lenient_cache = {}\n",
    "# resid_harsh_cache = {}\n",
    "\n",
    "for key in lenient_cache.keys():\n",
    "    if key.endswith(\"hook_resid_post\"):\n",
    "        resid_lenient_cache[key] = lenient_cache[key]\n",
    "    lenient_cache[key].cpu()\n",
    "# for key in harsh_cache.keys():\n",
    "    # if key.endswith(\"hook_resid_post\"):\n",
    "#         resid_harsh_cache[key] = harsh_cache[key]\n",
    "#     harsh_cache[key].cpu()\n",
    "\n",
    "del lenient_cache\n",
    "# del harsh_cache\n",
    "torch.cuda.empty_cache()\n",
    "#for layer in range(hooked_model.cfg.n_layers):\n",
    "#    resid_lenient_cache[layer] = \n",
    "# clean_lenient_logit_prob = lenient_logits[0, -1, clean_token]\n",
    "# toxic_lenient_logit_prob = lenient_logits[0, -1, toxic_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patching_hook(\n",
    "        activation,\n",
    "        hook,\n",
    "        cache,\n",
    "        position,\n",
    "):\n",
    "    activation[:, position, :] = cache[hook.name][:, position, :]\n",
    "    return activation\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_toxic_logit_diff(logits):\n",
    "    # clean - toxic\n",
    "    return logits[0, -1, 315] - logits[0, -1, 7495]\n",
    "\n",
    "def compare_logit_diffs(logits_a, logits_b):\n",
    "    diff_a = clean_toxic_logit_diff(logits_a)\n",
    "    diff_b = clean_toxic_logit_diff(logits_b)\n",
    "    return diff_a, diff_b\n",
    "\n",
    "# Clean token: tensor([[   1,  315, 1307, 2190]], device='cuda:0')\n",
    "# Toxic token: tensor([[    1,  7495, 29990,  2965]], device='cuda:0')\n",
    "# Clean str token: ['<s>', 'C', 'LE', 'AN']\n",
    "# Toxic str token: ['<s>', 'TO', 'X', 'IC']\n",
    "\n",
    "results = []\n",
    "\n",
    "for layer in tqdm(range(hooked_model.cfg.n_layers)):\n",
    "    hooked_model.reset_hooks()\n",
    "    temp_hook = partial(\n",
    "        patching_hook,\n",
    "        cache=resid_lenient_cache,\n",
    "        # cache=resid_harsh_cache,\n",
    "        position=-1,\n",
    "    )\n",
    "    hooked_model.blocks[layer].hook_resid_post.add_hook(temp_hook)\n",
    "    with torch.no_grad():\n",
    "        harsh_to_lenient_logits = hooked_model(harsh_tokens).to(\"cpu\")\n",
    "        # lenient_to_harsh_logits = hooked_model(lenient_tokens).to(\"cpu\")\n",
    "\n",
    "        # pre_soft_logit_diff_change = compare_logit_diffs(lenient_logits, harsh_to_lenient_logits)\n",
    "        # soft_logit_diff_change = compare_logit_diffs(lenient_logits.softmax(dim=-1), harsh_to_lenient_logits.softmax(dim=-1))\n",
    "\n",
    "        pre_soft_logit_diff_change = clean_toxic_logit_diff(harsh_to_lenient_logits).item()\n",
    "        # pre_soft_logit_diff_change = clean_toxic_logit_diff(lenient_to_harsh_logits).item()\n",
    "\n",
    "    # results.append((pre_soft_logit_diff_change.item(), soft_logit_diff_change.item()))\n",
    "    # results.append((pre_soft_logit_diff_change, soft_logit_diff_change))\n",
    "    results.append(pre_soft_logit_diff_change)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.title(\"Patching lenient model to be harsh\")\n",
    "plt.title(\"Patching harsh model to be lenient\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Logit diff (clean - toxic)\")\n",
    "plt.plot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
